\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={STA380 Exercises},
            pdfauthor={George Chen},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{STA380 Exercises}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{George Chen}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{8/16/2019}


\begin{document}
\maketitle

\hypertarget{green-buildings}{%
\section{Green Buildings}\label{green-buildings}}

For this question, we will be checking our excel guru's assumptions,
namely: * There is a ``green premium'' so that rent per square foot is
higher in green buildings * The premium remains constant over 8 years
(the ROI assuming a 90\% occupancy rate) * Occupancy rates are the same
between green/normal buildings * Do electricity costs remain constant
across green/normal buildings

I'll start by ignoring the buildings with low occupancy rates
(\textless{}10\% occupied)

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\NormalTok{greenbuildings <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{'data/greenbuildings.csv'}\NormalTok{, }\DataTypeTok{header=}\OtherTok{TRUE}\NormalTok{)}
\NormalTok{greenbuildings <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(greenbuildings)}
\NormalTok{greenbuildings <-}\StringTok{ }\NormalTok{greenbuildings }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(leasing_rate}\OperatorTok{>}\DecValTok{10}\NormalTok{)}
\NormalTok{greenbuildings}\OperatorTok{$}\NormalTok{green_rating <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(greenbuildings}\OperatorTok{$}\NormalTok{green_rating)}
\end{Highlighting}
\end{Shaded}

\hypertarget{how-much-is-the-green-premium}{%
\subsection{How much is the green
premium?}\label{how-much-is-the-green-premium}}

\hypertarget{does-it-remain-constant-within-the-first-10-years}{%
\subsection{Does it remain constant within the first 10
years?}\label{does-it-remain-constant-within-the-first-10-years}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df1 <-}\StringTok{ }\NormalTok{greenbuildings }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(age}\OperatorTok{<=}\DecValTok{10}\NormalTok{)}
\KeywordTok{ggplot}\NormalTok{(df1, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{age,}\DataTypeTok{y=}\NormalTok{Rent,}\DataTypeTok{fill=}\NormalTok{green_rating)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{stat=}\StringTok{'identity'}\NormalTok{, }\DataTypeTok{position=}\KeywordTok{position_dodge}\NormalTok{(), }\DataTypeTok{width =} \FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA380_exercise_George_files/figure-latex/unnamed-chunk-2-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{green <-}\StringTok{ }\NormalTok{greenbuildings }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(age}\OperatorTok{<=}\DecValTok{10}\NormalTok{, green_rating }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{group_by}\NormalTok{(age) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{avg=}\KeywordTok{mean}\NormalTok{(Rent))}

\NormalTok{normal <-}\StringTok{ }\NormalTok{greenbuildings }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(age}\OperatorTok{<=}\DecValTok{10}\NormalTok{, green_rating }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{group_by}\NormalTok{(age) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{avg=}\KeywordTok{mean}\NormalTok{(Rent))}

\KeywordTok{mean}\NormalTok{(green}\OperatorTok{$}\NormalTok{avg}\OperatorTok{-}\NormalTok{normal}\OperatorTok{$}\NormalTok{avg)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.9830872
\end{verbatim}

It seems that there isn't a green premium for newer buildings, and green
buildings actually recieve \$-0.98 less rent on average. \#\# How does
the occupancy rate change over time?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df1 <-}\StringTok{ }\NormalTok{greenbuildings }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(age}\OperatorTok{<=}\DecValTok{10}\NormalTok{)}
\KeywordTok{ggplot}\NormalTok{(df1, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{age,}\DataTypeTok{y=}\NormalTok{leasing_rate,}\DataTypeTok{fill=}\NormalTok{green_rating)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{stat=}\StringTok{'identity'}\NormalTok{, }\DataTypeTok{position=}\KeywordTok{position_dodge}\NormalTok{(), }\DataTypeTok{width =} \FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA380_exercise_George_files/figure-latex/unnamed-chunk-4-1.pdf}
There is no visible difference in leasing rate, except for the first
year.

\hypertarget{are-there-differences-in-electricty-costs}{%
\subsection{\texorpdfstring{Are there differences in electricty costs?
}{Are there differences in electricty costs?  }}\label{are-there-differences-in-electricty-costs}}

One of the selling points of green buildings is that even though the
cost to build, the savings in electricity costs can partially cover the
additional initial investment. Even though the dataset doesn't contain
electricy cost for individual buildings, we can still check this
assumption by checking if there is a higher percentage of green
buildings in regions with higher `degree days'.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df1 =}\StringTok{ }\NormalTok{greenbuildings }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(age}\OperatorTok{<=}\DecValTok{10}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{group_by}\NormalTok{(total_dd_}\DecValTok{07}\NormalTok{,green_rating) }\OperatorTok{%>%}\StringTok{ }
\StringTok{                        }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{n=}\KeywordTok{n}\NormalTok{()) }
\NormalTok{df1}\OperatorTok{$}\NormalTok{dd_quartile =}\StringTok{ }\KeywordTok{ntile}\NormalTok{(df1}\OperatorTok{$}\NormalTok{total_dd_}\DecValTok{07}\NormalTok{,}\DecValTok{4}\NormalTok{)}
\KeywordTok{ggplot}\NormalTok{(df1,}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{dd_quartile,}\DataTypeTok{y=}\NormalTok{n,}\DataTypeTok{fill=}\NormalTok{green_rating)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{position=}\StringTok{'fill'}\NormalTok{, }\DataTypeTok{stat=}\StringTok{'identity'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA380_exercise_George_files/figure-latex/unnamed-chunk-5-1.pdf}
It seems that the percentage of green buildings tend to increase with
the number of `degree days', except for the highest quartile. Our
explanation is that there's an incentive to go green because there are
some savings in electricity costs, as long as the weather isn't too
extreme. In conclusion, we disagree with the analysis done by our Excel
guru, because the green premium doesn't exist for newer buildigns, and
the assumption that he made about the occupancy rate doesn't stand
either. There is still an incentive to go green due to electricity cost
savings, but we believe that the magnitude is too small to justify the
initial investment in green certifications.

\hypertarget{flights-at-abia}{%
\section{Flights at ABIA}\label{flights-at-abia}}

For this question, we'll start by looking at the data from a consumer's
standpoint: * What are the distribution of delays by carrier, and by
month? * Which destinations have the largest probability of delays?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\NormalTok{flights =}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{'data/ABIA.csv'}\NormalTok{,}\DataTypeTok{stringsAsFactors =} \OtherTok{FALSE}\NormalTok{)}

\NormalTok{flights}\OperatorTok{$}\NormalTok{Month <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(flights}\OperatorTok{$}\NormalTok{Month)}
\NormalTok{flights}\OperatorTok{$}\NormalTok{DayofMonth <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(flights}\OperatorTok{$}\NormalTok{DayofMonth)}
\NormalTok{flights}\OperatorTok{$}\NormalTok{DayOfWeek <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(flights}\OperatorTok{$}\NormalTok{DayOfWeek)}
\end{Highlighting}
\end{Shaded}

We start by looking at the distribution of delays:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(flights, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{DepDelay)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth=}\DecValTok{30}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA380_exercise_George_files/figure-latex/unnamed-chunk-7-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(flights, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{ArrDelay)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth=}\DecValTok{30}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA380_exercise_George_files/figure-latex/unnamed-chunk-7-2.pdf}
Whether it is departure delays or arrival delays, planes usually are
within 10 minutes on schedule. \#\# Which months are better for
traveling?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dd_month =}\StringTok{ }\NormalTok{flights }\OperatorTok{%>%}
\StringTok{                  }\KeywordTok{group_by}\NormalTok{(Month) }\OperatorTok{%>%}
\StringTok{                  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean_dd =} \KeywordTok{mean}\NormalTok{(DepDelay,}\DataTypeTok{na.rm=}\OtherTok{TRUE}\NormalTok{),}\DataTypeTok{sd =} \KeywordTok{sd}\NormalTok{(DepDelay,}\DataTypeTok{na.rm=}\OtherTok{TRUE}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{                  }\KeywordTok{arrange}\NormalTok{(}\KeywordTok{desc}\NormalTok{(mean_dd))}

\NormalTok{dd_month}\OperatorTok{$}\NormalTok{Month <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(dd_month}\OperatorTok{$}\NormalTok{Month, }\DataTypeTok{levels =}\NormalTok{ dd_month}\OperatorTok{$}\NormalTok{Month[}\KeywordTok{order}\NormalTok{(}\OperatorTok{-}\NormalTok{dd_month}\OperatorTok{$}\NormalTok{mean_dd)])}

\KeywordTok{ggplot}\NormalTok{(dd_month, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Month, }\DataTypeTok{y=}\NormalTok{mean_dd)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{stat=}\StringTok{'identity'}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{      }\KeywordTok{geom_errorbar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{ymin=}\NormalTok{mean_dd}\OperatorTok{-}\NormalTok{sd,}\DataTypeTok{ymax=}\NormalTok{mean_dd}\OperatorTok{+}\NormalTok{sd))}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA380_exercise_George_files/figure-latex/unnamed-chunk-8-1.pdf}
Departures delays are worst during winter, spring, and summer vacations,
which makes sense because of the high amount of traffic during these
months. The spread of delays show similar patterns.

\hypertarget{which-carrier-has-the-worst-delays}{%
\subsection{Which carrier has the worst
delays?}\label{which-carrier-has-the-worst-delays}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights <-}\StringTok{ }\NormalTok{flights }\OperatorTok{%>%}
\StringTok{        }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{dep_type =} \KeywordTok{ifelse}\NormalTok{(DepDelay }\OperatorTok{<}\StringTok{ }\DecValTok{10}\NormalTok{, }\StringTok{"on time"}\NormalTok{, }\StringTok{"delayed"}\NormalTok{))}

\KeywordTok{ggplot}\NormalTok{(flights,}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{UniqueCarrier,}\DataTypeTok{y=}\NormalTok{DepDelay)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA380_exercise_George_files/figure-latex/unnamed-chunk-9-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(flights,}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{UniqueCarrier,}\DataTypeTok{y=}\NormalTok{DepDelay)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_boxplot}\NormalTok{(}\DataTypeTok{outlier.shape =} \OtherTok{NA}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{   }\KeywordTok{coord_cartesian}\NormalTok{(}\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{20}\NormalTok{,}\DecValTok{75}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA380_exercise_George_files/figure-latex/unnamed-chunk-9-2.pdf}
Most delays are within the 1-hour mark, with a few exceptionally long
delays. We remove the extreme outliers because for those cases,
passengers should be able to transfer flights or request a refund. We
would avoid ExpressJet, Blue Streak, and SouthWest airlines.

\hypertarget{which-destination-has-the-most-delays}{%
\subsection{Which destination has the most
delays?}\label{which-destination-has-the-most-delays}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights <-}\StringTok{ }\NormalTok{flights }\OperatorTok{%>%}
\StringTok{        }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{dep_type =} \KeywordTok{ifelse}\NormalTok{(DepDelay }\OperatorTok{<}\StringTok{ }\DecValTok{10}\NormalTok{, }\StringTok{"on time"}\NormalTok{, }\StringTok{"delayed"}\NormalTok{))}

\NormalTok{df_delay <-}\StringTok{ }\NormalTok{flights }\OperatorTok{%>%}
\StringTok{        }\KeywordTok{group_by}\NormalTok{(Dest) }\OperatorTok{%>%}
\StringTok{        }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{ot_dep_rate =} \KeywordTok{sum}\NormalTok{(dep_type }\OperatorTok{==}\StringTok{ "on time"}\NormalTok{) }\OperatorTok{/}\StringTok{ }\KeywordTok{n}\NormalTok{()) }\OperatorTok{%>%}
\StringTok{        }\KeywordTok{drop_na}\NormalTok{() }\OperatorTok{%>%}
\StringTok{        }\KeywordTok{arrange}\NormalTok{(}\KeywordTok{desc}\NormalTok{(ot_dep_rate))}

\KeywordTok{ggplot}\NormalTok{(df_delay, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\KeywordTok{reorder}\NormalTok{(Dest,}\OperatorTok{-}\NormalTok{ot_dep_rate), }\DataTypeTok{y=}\NormalTok{ot_dep_rate)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{stat=}\StringTok{'identity'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA380_exercise_George_files/figure-latex/unnamed-chunk-10-1.pdf}
Flights to Detroit, Tuscon, and Tulsa have the highest chance of delays.

\hypertarget{portfolio-modeling}{%
\subsection{Portfolio Modeling}\label{portfolio-modeling}}

For this question, we will look at three portfolios: 1. Healthcare /
Consumer goods = 3:2 allocation, industries that we feel are relatively
uneffected by the trade negotiations. 2. Full technology allocation,
industries that are in the spotlight because of the trade negotiatons.
3. Biotechnology / Real estate / Business services = 2:2:1 allocation,
our ``momentum'' portfolio because these industries have outperformed
the market for the past month.

We view portfolios 1 \& 2 as investments people will hold depending on
their outlooks of the trade negotiations, and portfolio 3 as the
holdings of someone who follows a momentum strategy.

However, we will look at the historical standard deviations of the
portfolios to check their riskiness:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(mosaic)}
\KeywordTok{library}\NormalTok{(foreach)}
\KeywordTok{library}\NormalTok{(quantmod)}
\end{Highlighting}
\end{Shaded}

Portfolio 1

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{portfolio_}\DecValTok{1}\NormalTok{ =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"XLV"}\NormalTok{,}\StringTok{"XBI"}\NormalTok{,}\StringTok{"IBB"}\NormalTok{,}\StringTok{"XRT"}\NormalTok{,}\StringTok{"XLY"}\NormalTok{)}
\KeywordTok{getSymbols}\NormalTok{(portfolio_}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "XLV" "XBI" "IBB" "XRT" "XLY"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{(ticker }\ControlFlowTok{in}\NormalTok{ portfolio_}\DecValTok{1}\NormalTok{) \{}
\NormalTok{  expr =}\StringTok{ }\KeywordTok{paste0}\NormalTok{(ticker, }\StringTok{"a = adjustOHLC("}\NormalTok{, ticker, }\StringTok{")"}\NormalTok{)}
  \KeywordTok{eval}\NormalTok{(}\KeywordTok{parse}\NormalTok{(}\DataTypeTok{text=}\NormalTok{expr))}
\NormalTok{\}}

\NormalTok{p1_returns =}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\KeywordTok{ClCl}\NormalTok{(XLVa),}\KeywordTok{ClCl}\NormalTok{(XBIa),}\KeywordTok{ClCl}\NormalTok{(IBBa),}\KeywordTok{ClCl}\NormalTok{(XRTa),}\KeywordTok{ClCl}\NormalTok{(XLYa))}
\NormalTok{p1_returns =}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(p1_returns))}
\end{Highlighting}
\end{Shaded}

Portfolio 2

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{portfolio_}\DecValTok{2}\NormalTok{ =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"XLK"}\NormalTok{,}\StringTok{"SMH"}\NormalTok{,}\StringTok{"SOXX"}\NormalTok{,}\StringTok{"IGV"}\NormalTok{,}\StringTok{"VGT"}\NormalTok{)}
\KeywordTok{getSymbols}\NormalTok{(portfolio_}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "XLK"  "SMH"  "SOXX" "IGV"  "VGT"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{(ticker }\ControlFlowTok{in}\NormalTok{ portfolio_}\DecValTok{2}\NormalTok{) \{}
\NormalTok{  expr =}\StringTok{ }\KeywordTok{paste0}\NormalTok{(ticker, }\StringTok{"a = adjustOHLC("}\NormalTok{, ticker, }\StringTok{")"}\NormalTok{)}
  \KeywordTok{eval}\NormalTok{(}\KeywordTok{parse}\NormalTok{(}\DataTypeTok{text=}\NormalTok{expr))}
\NormalTok{\}}

\NormalTok{p2_returns =}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\KeywordTok{ClCl}\NormalTok{(XLKa),}\KeywordTok{ClCl}\NormalTok{(SMHa),}\KeywordTok{ClCl}\NormalTok{(SOXXa),}\KeywordTok{ClCl}\NormalTok{(IGVa),}\KeywordTok{ClCl}\NormalTok{(VGTa))}
\NormalTok{p2_returns =}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(p2_returns))}
\end{Highlighting}
\end{Shaded}

Portfolio 3

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{portfolio_}\DecValTok{3}\NormalTok{ =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"XLV"}\NormalTok{,}\StringTok{"XBI"}\NormalTok{,}\StringTok{"IYR"}\NormalTok{,}\StringTok{"VNQ"}\NormalTok{,}\StringTok{"PTF"}\NormalTok{)}
\KeywordTok{getSymbols}\NormalTok{(portfolio_}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "XLV" "XBI" "IYR" "VNQ" "PTF"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{(ticker }\ControlFlowTok{in}\NormalTok{ portfolio_}\DecValTok{3}\NormalTok{) \{}
\NormalTok{  expr =}\StringTok{ }\KeywordTok{paste0}\NormalTok{(ticker, }\StringTok{"a = adjustOHLC("}\NormalTok{, ticker, }\StringTok{")"}\NormalTok{)}
  \KeywordTok{eval}\NormalTok{(}\KeywordTok{parse}\NormalTok{(}\DataTypeTok{text=}\NormalTok{expr))}
\NormalTok{\}}

\NormalTok{p3_returns =}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\KeywordTok{ClCl}\NormalTok{(XLVa),}\KeywordTok{ClCl}\NormalTok{(XBIa),}\KeywordTok{ClCl}\NormalTok{(IYRa),}\KeywordTok{ClCl}\NormalTok{(VNQa),}\KeywordTok{ClCl}\NormalTok{(PTFa))}
\NormalTok{p3_returns =}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(p3_returns))}
\end{Highlighting}
\end{Shaded}

We will now calculate the VaR using bootstrapping methods.

\hypertarget{portfolio-1---bearish-view-on-trade-negotiations}{%
\section{Portfolio 1 - Bearish view on trade
negotiations}\label{portfolio-1---bearish-view-on-trade-negotiations}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.rseed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{initial_wealth =}\StringTok{ }\DecValTok{100000}
\NormalTok{sim1 =}\StringTok{ }\KeywordTok{foreach}\NormalTok{(}\DataTypeTok{i=}\DecValTok{1}\OperatorTok{:}\DecValTok{5000}\NormalTok{, }\DataTypeTok{.combine=}\StringTok{'rbind'}\NormalTok{) }\OperatorTok{%do%}\StringTok{ }\NormalTok{\{}
\NormalTok{  total_wealth =}\StringTok{ }\NormalTok{initial_wealth}
\NormalTok{  weights =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.2}\NormalTok{)}
\NormalTok{  holdings =}\StringTok{ }\NormalTok{weights }\OperatorTok{*}\StringTok{ }\NormalTok{total_wealth}
\NormalTok{  n_days =}\StringTok{ }\DecValTok{20}
\NormalTok{  wealthtracker =}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, n_days)}
  \ControlFlowTok{for}\NormalTok{(today }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{n_days) \{}
\NormalTok{    return.today =}\StringTok{ }\KeywordTok{resample}\NormalTok{(p1_returns, }\DecValTok{1}\NormalTok{, }\DataTypeTok{orig.ids=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{    holdings =}\StringTok{ }\NormalTok{holdings }\OperatorTok{+}\StringTok{ }\NormalTok{holdings}\OperatorTok{*}\NormalTok{return.today}
\NormalTok{    total_wealth =}\StringTok{ }\KeywordTok{sum}\NormalTok{(holdings)}
\NormalTok{    wealthtracker[today] =}\StringTok{ }\NormalTok{total_wealth}
\NormalTok{  \}}
\NormalTok{  wealthtracker}
\NormalTok{\}}
\NormalTok{return1 =}\StringTok{ }\KeywordTok{mean}\NormalTok{(sim1[,n_days])}
\KeywordTok{hist}\NormalTok{(sim1[,n_days]}\OperatorTok{-}\StringTok{ }\NormalTok{initial_wealth, }\DataTypeTok{breaks=}\DecValTok{30}\NormalTok{,}\DataTypeTok{main =} \StringTok{"Histogram of Return for Portfolio 1"}\NormalTok{,}\DataTypeTok{xlab =} \StringTok{"Return"}\NormalTok{)}
\NormalTok{VaR1 =}\StringTok{ }\KeywordTok{quantile}\NormalTok{(sim1[,n_days], }\FloatTok{0.05}\NormalTok{) }\OperatorTok{-}\StringTok{ }\NormalTok{initial_wealth}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\NormalTok{VaR1,}\DataTypeTok{col=}\StringTok{"red"}\NormalTok{,}\DataTypeTok{lty=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA380_exercise_George_files/figure-latex/unnamed-chunk-15-1.pdf}
\# Portfolio 2 - Bullish view on trade negotiations

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.rseed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{initial_wealth =}\StringTok{ }\DecValTok{100000}
\NormalTok{sim2 =}\StringTok{ }\KeywordTok{foreach}\NormalTok{(}\DataTypeTok{i=}\DecValTok{1}\OperatorTok{:}\DecValTok{5000}\NormalTok{, }\DataTypeTok{.combine=}\StringTok{'rbind'}\NormalTok{) }\OperatorTok{%do%}\StringTok{ }\NormalTok{\{}
\NormalTok{  total_wealth =}\StringTok{ }\NormalTok{initial_wealth}
\NormalTok{  weights =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.2}\NormalTok{)}
\NormalTok{  holdings =}\StringTok{ }\NormalTok{weights }\OperatorTok{*}\StringTok{ }\NormalTok{total_wealth}
\NormalTok{  n_days =}\StringTok{ }\DecValTok{20}
\NormalTok{  wealthtracker =}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, n_days)}
  \ControlFlowTok{for}\NormalTok{(today }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{n_days) \{}
\NormalTok{    return.today =}\StringTok{ }\KeywordTok{resample}\NormalTok{(p2_returns, }\DecValTok{1}\NormalTok{, }\DataTypeTok{orig.ids=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{    holdings =}\StringTok{ }\NormalTok{holdings }\OperatorTok{+}\StringTok{ }\NormalTok{holdings}\OperatorTok{*}\NormalTok{return.today}
\NormalTok{    total_wealth =}\StringTok{ }\KeywordTok{sum}\NormalTok{(holdings)}
\NormalTok{    wealthtracker[today] =}\StringTok{ }\NormalTok{total_wealth}
\NormalTok{  \}}
\NormalTok{  wealthtracker}
\NormalTok{\}}
\NormalTok{return2 =}\StringTok{ }\KeywordTok{mean}\NormalTok{(sim2[,n_days])}
\KeywordTok{hist}\NormalTok{(sim2[,n_days]}\OperatorTok{-}\StringTok{ }\NormalTok{initial_wealth, }\DataTypeTok{breaks=}\DecValTok{30}\NormalTok{,}\DataTypeTok{main =} \StringTok{"Histogram of Return for Portfolio 1"}\NormalTok{,}\DataTypeTok{xlab =} \StringTok{"Return"}\NormalTok{)}
\NormalTok{VaR2 =}\StringTok{ }\KeywordTok{quantile}\NormalTok{(sim2[,n_days], }\FloatTok{0.05}\NormalTok{) }\OperatorTok{-}\StringTok{ }\NormalTok{initial_wealth}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\NormalTok{VaR2,}\DataTypeTok{col=}\StringTok{"red"}\NormalTok{,}\DataTypeTok{lty=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA380_exercise_George_files/figure-latex/unnamed-chunk-16-1.pdf}

\hypertarget{portfolio-3---momentum-investor}{%
\section{Portfolio 3 - Momentum
investor}\label{portfolio-3---momentum-investor}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.rseed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{initial_wealth =}\StringTok{ }\DecValTok{100000}
\NormalTok{sim3 =}\StringTok{ }\KeywordTok{foreach}\NormalTok{(}\DataTypeTok{i=}\DecValTok{1}\OperatorTok{:}\DecValTok{5000}\NormalTok{, }\DataTypeTok{.combine=}\StringTok{'rbind'}\NormalTok{) }\OperatorTok{%do%}\StringTok{ }\NormalTok{\{}
\NormalTok{  total_wealth =}\StringTok{ }\NormalTok{initial_wealth}
\NormalTok{  weights =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.2}\NormalTok{)}
\NormalTok{  holdings =}\StringTok{ }\NormalTok{weights }\OperatorTok{*}\StringTok{ }\NormalTok{total_wealth}
\NormalTok{  n_days =}\StringTok{ }\DecValTok{20}
\NormalTok{  wealthtracker =}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, n_days)}
  \ControlFlowTok{for}\NormalTok{(today }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{n_days) \{}
\NormalTok{    return.today =}\StringTok{ }\KeywordTok{resample}\NormalTok{(p3_returns, }\DecValTok{1}\NormalTok{, }\DataTypeTok{orig.ids=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{    holdings =}\StringTok{ }\NormalTok{holdings }\OperatorTok{+}\StringTok{ }\NormalTok{holdings}\OperatorTok{*}\NormalTok{return.today}
\NormalTok{    total_wealth =}\StringTok{ }\KeywordTok{sum}\NormalTok{(holdings)}
\NormalTok{    wealthtracker[today] =}\StringTok{ }\NormalTok{total_wealth}
\NormalTok{  \}}
\NormalTok{  wealthtracker}
\NormalTok{\}}
\NormalTok{return3 =}\StringTok{ }\KeywordTok{mean}\NormalTok{(sim3[,n_days])}
\KeywordTok{hist}\NormalTok{(sim3[,n_days]}\OperatorTok{-}\StringTok{ }\NormalTok{initial_wealth, }\DataTypeTok{breaks=}\DecValTok{30}\NormalTok{,}\DataTypeTok{main =} \StringTok{"Histogram of Return for Portfolio 1"}\NormalTok{,}\DataTypeTok{xlab =} \StringTok{"Return"}\NormalTok{)}
\NormalTok{VaR3 =}\StringTok{ }\KeywordTok{quantile}\NormalTok{(sim3[,n_days], }\FloatTok{0.05}\NormalTok{) }\OperatorTok{-}\StringTok{ }\NormalTok{initial_wealth}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v=}\NormalTok{VaR3,}\DataTypeTok{col=}\StringTok{"red"}\NormalTok{,}\DataTypeTok{lty=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA380_exercise_George_files/figure-latex/unnamed-chunk-17-1.pdf}
We feel under current market conditions, calculating VaR with
bootstrapping should be better than using Z-scores, due to the amount of
market-moving news stories that are popping up every week, which causes
the normality assumptions to fail. Also regarding the risks of our
portfolios: Portfolio 1 risks 1 dollar to get 0.204 dollars Portfolio 2
risks 1 dollar to get 0.124 dollars Portfolio 3 risks 1 dollar to get
0.146 dollars If we were to enter a trading competition today, we would
probably be looking for an industry allocation similar to portfolio 1.

\hypertarget{market-segmentation}{%
\section{Market segmentation}\label{market-segmentation}}

For this question, we'll try to segment NutrientH20's twitter followers
using PCA, and provide some online marketing suggestions. First we
removed ``chatter'', ``uncategorized'', ``adult'', and ``spam'' columns
because those variables do not provide additional insight in their
current state. We then calculate the frequency of topics for each
twitter follower.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\NormalTok{social <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{'data/social_marketing.csv'}\NormalTok{, }\DataTypeTok{header=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{row.names=}\DecValTok{1}\NormalTok{)}
\NormalTok{social <-}\StringTok{ }\NormalTok{social[,}\OperatorTok{-}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{35}\NormalTok{,}\DecValTok{36}\NormalTok{)]}
\NormalTok{social_freq =}\StringTok{ }\NormalTok{social}\OperatorTok{/}\KeywordTok{rowSums}\NormalTok{(social)}
\end{Highlighting}
\end{Shaded}

We decide to analyze the top 9 loadings, because together they explain
half of the total variance in our dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pca =}\StringTok{ }\KeywordTok{prcomp}\NormalTok{(social_freq, }\DataTypeTok{scale=}\OtherTok{TRUE}\NormalTok{)}
\NormalTok{loadings =}\StringTok{ }\NormalTok{pca}\OperatorTok{$}\NormalTok{rotation}
\NormalTok{scores =}\StringTok{ }\NormalTok{pca}\OperatorTok{$}\NormalTok{x}
\NormalTok{s =}\StringTok{ }\KeywordTok{summary}\NormalTok{(pca)}
\KeywordTok{plot}\NormalTok{(s}\OperatorTok{$}\NormalTok{importance[}\DecValTok{3}\NormalTok{,],}\DataTypeTok{xlab=}\StringTok{'PC'}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{'Cumulative Proportion'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA380_exercise_George_files/figure-latex/unnamed-chunk-19-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{9}\NormalTok{) \{}
\NormalTok{  expr =}\StringTok{ }\KeywordTok{paste0}\NormalTok{(}\StringTok{"o"}\NormalTok{,i,}\StringTok{" = order(loadings[,"}\NormalTok{,i,}\StringTok{"],decreasing=TRUE)"}\NormalTok{)}
  \KeywordTok{eval}\NormalTok{(}\KeywordTok{parse}\NormalTok{(}\DataTypeTok{text=}\NormalTok{expr))}
\NormalTok{\}}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{9}\NormalTok{) \{}
\NormalTok{  expr =}\StringTok{ }\KeywordTok{paste0}\NormalTok{(}\StringTok{"colnames(social_freq)[head(o"}\NormalTok{,i,}\StringTok{",3)]"}\NormalTok{)}
  \KeywordTok{print}\NormalTok{(}\KeywordTok{eval}\NormalTok{(}\KeywordTok{parse}\NormalTok{(}\DataTypeTok{text=}\NormalTok{expr)))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "sports_fandom" "religion"      "parenting"    
## [1] "health_nutrition" "personal_fitness" "outdoors"        
## [1] "fashion" "beauty"  "cooking"
## [1] "college_uni"    "online_gaming"  "sports_playing"
## [1] "shopping"       "photo_sharing"  "current_events"
## [1] "automotive"    "photo_sharing" "shopping"     
## [1] "automotive" "news"       "tv_film"   
## [1] "music"   "cooking" "food"   
## [1] "music"          "small_business" "business"
\end{verbatim}

We will now group by the top variable for each component, and list our
suggestions for marketing recommendations for each group: * Group 1 -
Parents: Sponsor children's sports events * Group 2 - Fitness freak:
Bottle should fit easily in the side pockets of backpacks * Group 3 -
Beauty blogger: Focus on how water can make your skin glow * Group 4 -
College student: Offer bulk quanitity discounts * Group 5 - Activist:
Consider green packaging * Group 6 - Car enthusiast: Give discounts to
car dealers * Group 7 - Music lover: Mention NutrientH20 isn't a part of
a large corporation

\hypertarget{author-attribution}{%
\subsection{Author attribution}\label{author-attribution}}

For this question, we will be predicting the author of an article based
on textual content, we will be fitting three models: * Logistic
regression with lasso regulization * Random Forest * Naive bayes First
we will read in the training data and create a corpus

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tm) }
\KeywordTok{library}\NormalTok{(magrittr)}
\KeywordTok{library}\NormalTok{(slam)}
\KeywordTok{library}\NormalTok{(proxy)}
\KeywordTok{library}\NormalTok{(glmnet)}

\CommentTok{# Reader function that specifies English}
\NormalTok{readerPlain =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(fname)\{}
  \KeywordTok{readPlain}\NormalTok{(}\DataTypeTok{elem=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{content=}\KeywordTok{readLines}\NormalTok{(fname)), }
            \DataTypeTok{id=}\NormalTok{fname, }\DataTypeTok{language=}\StringTok{'en'}\NormalTok{) \}}

\CommentTok{# Read the training data and create a corpus}
\KeywordTok{setwd}\NormalTok{(}\StringTok{"data/ReutersC50/C50train"}\NormalTok{)}
\NormalTok{doc_list =}\StringTok{ }\KeywordTok{Sys.glob}\NormalTok{(}\StringTok{'*'}\NormalTok{)}
\NormalTok{file_list =}\StringTok{ }\KeywordTok{Sys.glob}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(doc_list, }\StringTok{'/*.txt'}\NormalTok{))}

\CommentTok{# Clean up the filenames, }
\NormalTok{all_docs =}\StringTok{ }\KeywordTok{lapply}\NormalTok{(file_list, readerPlain) }
\NormalTok{mynames =}\StringTok{ }\NormalTok{file_list }\OperatorTok{%>%}
\NormalTok{\{ }\KeywordTok{strsplit}\NormalTok{(., }\StringTok{'/'}\NormalTok{, }\DataTypeTok{fixed=}\OtherTok{TRUE}\NormalTok{) \} }\OperatorTok{%>%}
\NormalTok{\{ }\KeywordTok{lapply}\NormalTok{(., tail, }\DataTypeTok{n=}\DecValTok{2}\NormalTok{) \} }\OperatorTok{%>%}
\NormalTok{\{ }\KeywordTok{lapply}\NormalTok{(., paste0, }\DataTypeTok{collapse =} \StringTok{''}\NormalTok{) \} }\OperatorTok{%>%}
\StringTok{  }\NormalTok{unlist}

\KeywordTok{names}\NormalTok{(all_docs) =}\StringTok{ }\NormalTok{mynames}
\NormalTok{documents_raw =}\StringTok{ }\KeywordTok{VCorpus}\NormalTok{(}\KeywordTok{VectorSource}\NormalTok{(all_docs))}
\end{Highlighting}
\end{Shaded}

Preprocess the data with the following steps: 1. Make everything
lowercase 2. Remove numbers 3. Remove punctuation 4. Remove excess
whitespace 5. Remove stopwords 6. Combine stem words

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_documents =}\StringTok{ }\NormalTok{documents_raw}
\NormalTok{my_documents =}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(my_documents, }\KeywordTok{content_transformer}\NormalTok{(tolower)) }
\NormalTok{my_documents =}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(my_documents, }\KeywordTok{content_transformer}\NormalTok{(removeNumbers)) }
\NormalTok{my_documents =}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(my_documents, }\KeywordTok{content_transformer}\NormalTok{(removePunctuation)) }
\NormalTok{my_documents =}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(my_documents, }\KeywordTok{content_transformer}\NormalTok{(stripWhitespace))}
\NormalTok{my_documents =}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(my_documents, }\KeywordTok{content_transformer}\NormalTok{(removeWords), }\KeywordTok{stopwords}\NormalTok{(}\StringTok{"SMART"}\NormalTok{)) }\CommentTok{##}
\NormalTok{my_documents =}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(my_documents, stemDocument)}
\end{Highlighting}
\end{Shaded}

Now we convert the corpus into a document-term matrix(DTM), and then
drop terms that have count 0 in less than 95\% of documents.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DTM =}\StringTok{ }\KeywordTok{DocumentTermMatrix}\NormalTok{(my_documents)}

\NormalTok{DTM =}\StringTok{ }\KeywordTok{removeSparseTerms}\NormalTok{(DTM, }\FloatTok{0.95}\NormalTok{)}

\NormalTok{tfidf =}\StringTok{ }\KeywordTok{weightTfIdf}\NormalTok{(DTM)}
\end{Highlighting}
\end{Shaded}

Now we run PCA and pick the top 150 principal components, which account
for 50\% of the total variance.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#PCA on tfidf}
\NormalTok{X =}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(tfidf)}
\NormalTok{pca=}\StringTok{ }\KeywordTok{prcomp}\NormalTok{(X, }\DataTypeTok{scale=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(pca)}\OperatorTok{$}\NormalTok{importance[}\DecValTok{3}\NormalTok{,]}\OperatorTok{%>%}\KeywordTok{plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA380_exercise_George_files/figure-latex/unnamed-chunk-24-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#independent variables: X}
\NormalTok{X =}\StringTok{ }\NormalTok{pca}\OperatorTok{$}\NormalTok{x[,}\DecValTok{1}\OperatorTok{:}\DecValTok{150}\NormalTok{]}

\NormalTok{all  <-}\StringTok{ }\KeywordTok{vector}\NormalTok{()}
\NormalTok{i =}\StringTok{ }\DecValTok{1}
\ControlFlowTok{for}\NormalTok{ (auther }\ControlFlowTok{in}\NormalTok{ doc_list)\{}
\NormalTok{  all[i] =}\StringTok{ }\NormalTok{auther}
\NormalTok{  i =}\StringTok{ }\NormalTok{i}\OperatorTok{+}\DecValTok{1}
\NormalTok{\}}

\CommentTok{#Model dependent variable: Y}
\NormalTok{Y =}\StringTok{ }\KeywordTok{vector}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{( i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{2500}\NormalTok{)\{}
\NormalTok{  Y[i] =}\StringTok{ }\NormalTok{all[}\KeywordTok{ceiling}\NormalTok{(i}\OperatorTok{/}\DecValTok{50}\NormalTok{)]}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now do the same transformations for the test set:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{readerPlain =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(fname)\{}
  \KeywordTok{readPlain}\NormalTok{(}\DataTypeTok{elem=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{content=}\KeywordTok{readLines}\NormalTok{(fname)), }
            \DataTypeTok{id=}\NormalTok{fname, }\DataTypeTok{language=}\StringTok{'en'}\NormalTok{) \}}
\KeywordTok{setwd}\NormalTok{(}\StringTok{"data/ReutersC50/C50test"}\NormalTok{)}
\NormalTok{doc_list =}\StringTok{ }\KeywordTok{Sys.glob}\NormalTok{(}\StringTok{'*'}\NormalTok{)}
\NormalTok{file_list =}\StringTok{ }\KeywordTok{Sys.glob}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(doc_list, }\StringTok{'/*.txt'}\NormalTok{))}


\NormalTok{file_list =}\StringTok{ }\KeywordTok{Sys.glob}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(doc_list, }\StringTok{'/*.txt'}\NormalTok{))}
\NormalTok{cp2 =}\StringTok{ }\KeywordTok{lapply}\NormalTok{(file_list, readerPlain) }


\NormalTok{mynames =}\StringTok{ }\NormalTok{file_list }\OperatorTok{%>%}
\NormalTok{\{ }\KeywordTok{strsplit}\NormalTok{(., }\StringTok{'/'}\NormalTok{, }\DataTypeTok{fixed=}\OtherTok{TRUE}\NormalTok{) \} }\OperatorTok{%>%}
\NormalTok{\{ }\KeywordTok{lapply}\NormalTok{(., tail, }\DataTypeTok{n=}\DecValTok{2}\NormalTok{) \} }\OperatorTok{%>%}
\NormalTok{\{ }\KeywordTok{lapply}\NormalTok{(., paste0, }\DataTypeTok{collapse =} \StringTok{''}\NormalTok{) \} }\OperatorTok{%>%}
\StringTok{  }\NormalTok{unlist}
\KeywordTok{names}\NormalTok{(cp2) =}\StringTok{ }\NormalTok{mynames}


\NormalTok{documents_raw_}\DecValTok{1}\NormalTok{ =}\StringTok{ }\KeywordTok{VCorpus}\NormalTok{(}\KeywordTok{VectorSource}\NormalTok{(cp2))}

\NormalTok{my_documents1 =}\StringTok{ }\NormalTok{documents_raw_}\DecValTok{1}
\NormalTok{my_documents1 =}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(my_documents1, }\KeywordTok{content_transformer}\NormalTok{(tolower))}
\NormalTok{my_documents1 =}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(my_documents1, }\KeywordTok{content_transformer}\NormalTok{(removeNumbers))}
\NormalTok{my_documents1 =}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(my_documents1, }\KeywordTok{content_transformer}\NormalTok{(removePunctuation)) }
\NormalTok{my_documents1 =}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(my_documents1, }\KeywordTok{content_transformer}\NormalTok{(stripWhitespace)) }
\NormalTok{my_documents1 =}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(my_documents1, }\KeywordTok{content_transformer}\NormalTok{(removeWords), }\KeywordTok{stopwords}\NormalTok{(}\StringTok{"SMART"}\NormalTok{))}
\NormalTok{my_documents1 =}\StringTok{ }\KeywordTok{tm_map}\NormalTok{(my_documents1, stemDocument)}

\NormalTok{DTM_test =}\StringTok{ }\KeywordTok{DocumentTermMatrix}\NormalTok{(my_documents1,}\DataTypeTok{control =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{dictionary=}\KeywordTok{Terms}\NormalTok{(DTM)))}
\NormalTok{DTM_test =}\StringTok{ }\KeywordTok{removeSparseTerms}\NormalTok{(DTM_test, }\FloatTok{0.95}\NormalTok{)}

\NormalTok{tfidf_test =}\StringTok{ }\KeywordTok{weightTfIdf}\NormalTok{(DTM_test)}

\NormalTok{X_test =}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(tfidf_test)}
\end{Highlighting}
\end{Shaded}

Some terms in the test set are not in the training set, we will be
ignoring these words.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Matching column names of test TFIDF to train TFIDF}
\NormalTok{train_pre_pc =}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(tfidf)}

\NormalTok{train_name =}\StringTok{ }\KeywordTok{colnames}\NormalTok{(train_pre_pc)}
\NormalTok{test_name =}\StringTok{ }\KeywordTok{colnames}\NormalTok{(X_test)}
\NormalTok{sup =}\StringTok{ }\KeywordTok{setdiff}\NormalTok{(train_name, test_name)}

\NormalTok{temp_x =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(X_test)}
\ControlFlowTok{for}\NormalTok{ (colname_ }\ControlFlowTok{in}\NormalTok{ sup)\{}
\NormalTok{  temp_x[,colname_] =}\StringTok{ }\DecValTok{0}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Somehow ``break.'' doesn't get its punctuation removed properly so we
will fix it manually. Afterwards, we will transform the test set to the
PC spaces of the training set.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#setdiff(colnames(t), train_name)}

\KeywordTok{colnames}\NormalTok{(temp_x)[}\KeywordTok{colnames}\NormalTok{(temp_x)}\OperatorTok{==}\StringTok{"break."}\NormalTok{] <-}\StringTok{ "break"}
\NormalTok{t =}\StringTok{ }\KeywordTok{data.matrix}\NormalTok{(temp_x)}
\NormalTok{t <-}\StringTok{ }\NormalTok{t[, }\KeywordTok{order}\NormalTok{(}\KeywordTok{colnames}\NormalTok{(t))]}

\CommentTok{#transform the test set to the principal component spaces of the training set}
\NormalTok{test.data <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(pca, }\DataTypeTok{newdata =}\NormalTok{t)}
\NormalTok{test.data <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(test.data)}
\NormalTok{test.data <-}\StringTok{ }\NormalTok{test.data[,}\DecValTok{1}\OperatorTok{:}\DecValTok{150}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

First, we will try Logistic regression with lasso regulization:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'caret' was built under R version 3.6.1
\end{verbatim}

\begin{verbatim}
## 
## Attaching package: 'caret'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:mosaic':
## 
##     dotPlot
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:purrr':
## 
##     lift
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(glmnet)}
\CommentTok{#out1 = glmnet(X, factor(Y), family="multinomial")}
\NormalTok{cv_lr =}\StringTok{ }\KeywordTok{cv.glmnet}\NormalTok{(X, }\KeywordTok{factor}\NormalTok{(Y), }\DataTypeTok{family=}\StringTok{"multinomial"}\NormalTok{,}\DataTypeTok{alpha=}\DecValTok{1}\NormalTok{)}
\NormalTok{p1 =}\StringTok{ }\KeywordTok{predict}\NormalTok{(cv_lr, }\KeywordTok{data.matrix}\NormalTok{(test.data), }\DataTypeTok{s=}\StringTok{"lambda.min"}\NormalTok{, }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}

\NormalTok{myPredict_for_lr <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(which_article)\{}
  \KeywordTok{return}\NormalTok{(}\KeywordTok{which.max}\NormalTok{(p1[which_article,,]))}
\NormalTok{\}}

\NormalTok{Ya  <-}\StringTok{ }\KeywordTok{vector}\NormalTok{()}
\NormalTok{i =}\StringTok{ }\DecValTok{1}
\ControlFlowTok{for}\NormalTok{ (author }\ControlFlowTok{in}\NormalTok{ doc_list)\{}
\NormalTok{  Ya[i] =}\StringTok{ }\NormalTok{author}
\NormalTok{  i =}\StringTok{ }\NormalTok{i}\OperatorTok{+}\DecValTok{1}
\NormalTok{\}}

\CommentTok{#y_test are the true author names}
\NormalTok{y_test =}\StringTok{ }\KeywordTok{vector}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{( i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{2500}\NormalTok{)\{}
\NormalTok{  y_test[i] =}\StringTok{ }\NormalTok{Ya[}\KeywordTok{ceiling}\NormalTok{(i}\OperatorTok{/}\DecValTok{50}\NormalTok{)]}
\NormalTok{\}}

\CommentTok{#Predict and calculate accuracy}
\NormalTok{lr_pred =}\StringTok{ }\KeywordTok{vector}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{2500}\NormalTok{)\{}
\NormalTok{  lr_pred[i] =}\KeywordTok{names}\NormalTok{(}\KeywordTok{myPredict_for_lr}\NormalTok{(i))}
\NormalTok{\}}

\KeywordTok{mean}\NormalTok{(lr_pred }\OperatorTok{==}\StringTok{ }\KeywordTok{factor}\NormalTok{(y_test))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.55
\end{verbatim}

Second, we will try random forest:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(randomForest)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'randomForest' was built under R version 3.6.1
\end{verbatim}

\begin{verbatim}
## randomForest 4.6-14
\end{verbatim}

\begin{verbatim}
## Type rfNews() to see new features/changes/bug fixes.
\end{verbatim}

\begin{verbatim}
## 
## Attaching package: 'randomForest'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:dplyr':
## 
##     combine
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:ggplot2':
## 
##     margin
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fY =}\StringTok{ }\KeywordTok{factor}\NormalTok{(Y)}
\NormalTok{dfX =}\KeywordTok{data.frame}\NormalTok{(X)}
\NormalTok{XY =}\StringTok{ }\KeywordTok{cbind}\NormalTok{(dfX, fY)}

\NormalTok{rffit =}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(fY}\OperatorTok{~}\NormalTok{.,}\DataTypeTok{data=}\NormalTok{XY,}\DataTypeTok{ntree=}\DecValTok{500}\NormalTok{)}
\NormalTok{rf_pred =}\StringTok{ }\KeywordTok{predict}\NormalTok{(rffit, }\DataTypeTok{newdata =}\NormalTok{ test.data)}
\KeywordTok{mean}\NormalTok{(rf_pred }\OperatorTok{==}\StringTok{ }\KeywordTok{factor}\NormalTok{(y_test))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5088
\end{verbatim}

Finally we try naive bayes:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(e1071)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'e1071' was built under R version 3.6.1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nb =}\StringTok{ }\KeywordTok{naiveBayes}\NormalTok{(fY}\OperatorTok{~}\NormalTok{.,}\DataTypeTok{data=}\NormalTok{XY)}
\NormalTok{nb_pred =}\StringTok{ }\KeywordTok{predict}\NormalTok{(nb, }\DataTypeTok{newdata =}\NormalTok{ test.data)}
\KeywordTok{mean}\NormalTok{(nb_pred }\OperatorTok{==}\StringTok{ }\KeywordTok{factor}\NormalTok{(y_test))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4204
\end{verbatim}

For our case, logistic regression with lasso regulation seems to work
best, which gives us 55\% accuracy. Random forest works okay as a
baseline because it doesn't require a lot of tuning (it gives 51\%
accuracy), and naive bayes doesn't work very well, which gives us 42\%
accuracy, probably because Reuters is a business \& finance news
website, and word choices in these articles may not be independant.
However, naive bayes could perform better then the other models as we
approach larger datasets and articles with more diverse topics, because
of its computational efficiency.

\hypertarget{association-rule-mining}{%
\subsection{Association rule mining}\label{association-rule-mining}}

For this question, we will utilize association rule mining to provide
product placement / promotional discounts / and advertisement
suggestions for our mock grocery store.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(arules)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'arules' was built under R version 3.6.1
\end{verbatim}

\begin{verbatim}
## 
## Attaching package: 'arules'
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:tm':
## 
##     inspect
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:mosaic':
## 
##     inspect, lhs, rhs
\end{verbatim}

\begin{verbatim}
## The following object is masked from 'package:dplyr':
## 
##     recode
\end{verbatim}

\begin{verbatim}
## The following objects are masked from 'package:base':
## 
##     abbreviate, write
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{groceries =}\StringTok{ }\KeywordTok{read.transactions}\NormalTok{(}\StringTok{"data/groceries.txt"}\NormalTok{, }\DataTypeTok{format=}\StringTok{"basket"}\NormalTok{, }\DataTypeTok{sep=}\StringTok{","}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(groceries)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## transactions as itemMatrix in sparse format with
##  9835 rows (elements/itemsets/transactions) and
##  169 columns (items) and a density of 0.02609146 
## 
## most frequent items:
##       whole milk other vegetables       rolls/buns             soda 
##             2513             1903             1809             1715 
##           yogurt          (Other) 
##             1372            34055 
## 
## element (itemset/transaction) length distribution:
## sizes
##    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15 
## 2159 1643 1299 1005  855  645  545  438  350  246  182  117   78   77   55 
##   16   17   18   19   20   21   22   23   24   26   27   28   29   32 
##   46   29   14   14    9   11    4    6    1    1    1    1    3    1 
## 
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   1.000   2.000   3.000   4.409   6.000  32.000 
## 
## includes extended item information - examples:
##             labels
## 1 abrasive cleaner
## 2 artif. sweetener
## 3   baby cosmetics
\end{verbatim}

The summary shows that our mock grocery stores is more likely a 7-11
than a Wallmart due to the high number of customers that buy 1-2 items.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{itemFrequencyPlot}\NormalTok{(groceries, }\DataTypeTok{topN=}\DecValTok{15}\NormalTok{, }\DataTypeTok{type=}\StringTok{"absolute"}\NormalTok{, }\DataTypeTok{main=}\StringTok{"Item Frequency"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{STA380_exercise_George_files/figure-latex/unnamed-chunk-32-1.pdf}
From the item frequency plot, our grocery store should be locaetd near a
residential area.

We will explain the reasoning behind our thresholds: * support: We will
only consider patterns that appear in more than 0.5\% of our
transactions * confidence: We will only consider transactions where the
right-hand side appears in more than 60\% of transactions that contains
the items on the left-hand side. * maxlen: We choose length 3 because
transactions that contain 3 or less items already account for more than
50\% of all transactions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{groceries_trans <-}\StringTok{ }\KeywordTok{as}\NormalTok{(groceries, }\StringTok{"transactions"}\NormalTok{)}
\NormalTok{groceries_rules <-}\StringTok{ }\KeywordTok{apriori}\NormalTok{(groceries_trans, }\DataTypeTok{parameter=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{support=}\NormalTok{.}\DecValTok{005}\NormalTok{, }\DataTypeTok{confidence=}\NormalTok{.}\DecValTok{60}\NormalTok{, }\DataTypeTok{maxlen=}\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Apriori
## 
## Parameter specification:
##  confidence minval smax arem  aval originalSupport maxtime support minlen
##         0.6    0.1    1 none FALSE            TRUE       5   0.005      1
##  maxlen target   ext
##       3  rules FALSE
## 
## Algorithmic control:
##  filter tree heap memopt load sort verbose
##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE
## 
## Absolute minimum support count: 49 
## 
## set item appearances ...[0 item(s)] done [0.00s].
## set transactions ...[169 item(s), 9835 transaction(s)] done [0.00s].
## sorting and recoding items ... [120 item(s)] done [0.00s].
## creating transaction tree ... done [0.00s].
## checking subsets of size 1 2 3
\end{verbatim}

\begin{verbatim}
## Warning in apriori(groceries_trans, parameter = list(support = 0.005,
## confidence = 0.6, : Mining stopped (maxlen reached). Only patterns up to a
## length of 3 returned!
\end{verbatim}

\begin{verbatim}
##  done [0.00s].
## writing ... [13 rule(s)] done [0.00s].
## creating S4 object  ... done [0.00s].
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rules_by_lift <-}\StringTok{ }\KeywordTok{sort}\NormalTok{(groceries_rules, }\DataTypeTok{by =} \StringTok{"lift"}\NormalTok{)}
\KeywordTok{inspect}\NormalTok{(rules_by_lift)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      lhs                               rhs                support    
## [1]  {pip fruit,whipped/sour cream} => {other vegetables} 0.005592272
## [2]  {onions,root vegetables}       => {other vegetables} 0.005693950
## [3]  {butter,whipped/sour cream}    => {whole milk}       0.006710727
## [4]  {pip fruit,whipped/sour cream} => {whole milk}       0.005998983
## [5]  {butter,yogurt}                => {whole milk}       0.009354347
## [6]  {butter,root vegetables}       => {whole milk}       0.008235892
## [7]  {curd,tropical fruit}          => {whole milk}       0.006507372
## [8]  {domestic eggs,pip fruit}      => {whole milk}       0.005388917
## [9]  {butter,tropical fruit}        => {whole milk}       0.006202339
## [10] {domestic eggs,margarine}      => {whole milk}       0.005185562
## [11] {butter,domestic eggs}         => {whole milk}       0.005998983
## [12] {domestic eggs,tropical fruit} => {whole milk}       0.006914082
## [13] {bottled water,butter}         => {whole milk}       0.005388917
##      confidence lift     count
## [1]  0.6043956  3.123610 55   
## [2]  0.6021505  3.112008 56   
## [3]  0.6600000  2.583008 66   
## [4]  0.6483516  2.537421 59   
## [5]  0.6388889  2.500387 92   
## [6]  0.6377953  2.496107 81   
## [7]  0.6336634  2.479936 64   
## [8]  0.6235294  2.440275 53   
## [9]  0.6224490  2.436047 61   
## [10] 0.6219512  2.434099 51   
## [11] 0.6210526  2.430582 59   
## [12] 0.6071429  2.376144 68   
## [13] 0.6022727  2.357084 53
\end{verbatim}

* Rows 1 \& 2 looks like a shopping list for chili or salsa, we'd
suggest placing the ingrediants near to each other * Items from row 5
have similar expiration dates, so bundling them up and providing
discounts may drive revenue. * Bottled water sales seem a bit lackluster
considering that our grocery store is likely a 7-11, placing
advertisements or coupons near the most popular product (milk) may be
worth a try.


\end{document}
